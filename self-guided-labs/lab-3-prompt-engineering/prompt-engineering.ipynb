{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n\n# Prompt Notebook - Prompt Engineering Exercises\nThis notebook contains steps and code to demonstrate inferencing of prompts\ngenerated in Prompt Lab in watsonx.ai. It introduces Python API commands\nfor authentication using API key and prompt inferencing using WML API.\n\nSome familiarity with Python is helpful. This notebook uses Python 3.10.\n\n## Notebook goals\nThe learning goals of this notebook are:\n\n* Defining a Python function for obtaining credentials from the IBM Cloud personal API key\n* Defining parameters of the Model object\n* Using the Model object to generate response using the defined model id, parameters and the prompt input\n\n# Setup", "metadata": {}}, {"cell_type": "markdown", "source": "## watsonx API connection\nThis cell defines the credentials required to work with watsonx API for Foundation\nModel inferencing.", "metadata": {}}, {"cell_type": "code", "source": "import os\nimport getpass\n\ndef get_credentials():\n\treturn {\n\t\t\"url\" : \"https://us-south.ml.cloud.ibm.com\",\n\t\t\"apikey\" : getpass.getpass(\"Please enter your api key (hit enter): \")\n\t}\n", "metadata": {"id": "2af327fa-2de6-4c1b-8040-b5edb0be8c6a"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Inferencing\nThis cell demonstrated how we can use the model object as well as the created access token\nto pair it with parameters and input string to obtain\nthe response from the the selected foundation model.\n\n## Defining the model id\nWe need to specify model id that will be used for inferencing:\n", "metadata": {}}, {"cell_type": "code", "source": "model_id = \"meta-llama/llama-3-3-70b-instruct\"\n# model_id = \"ibm/granite-3-8b-instruct\"", "metadata": {"id": "68bf7396-c57b-4d7c-87f8-66b8bc8cfc1b"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Defining the model parameters\nWe need to provide a set of model parameters that will influence the\nresult:", "metadata": {}}, {"cell_type": "code", "source": "parameters = {\n    \"decoding_method\": \"greedy\",\n    \"max_new_tokens\": 200,\n    \"min_new_tokens\": 50,\n    \"repetition_penalty\": 1\n}", "metadata": {"id": "aaa3dc06-14b4-43eb-8cfa-ee702cfbac2d"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Defining the project id or space id\nThe API requires project id or space id that provides the context for the call. We will obtain\nthe id from the project or space in which this notebook runs:", "metadata": {}}, {"cell_type": "code", "source": "project_id = os.getenv(\"PROJECT_ID\")\nspace_id = os.getenv(\"SPACE_ID\")\n", "metadata": {"id": "f20cb211-ea64-4822-87cc-9c9a67ff49cf"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Defining the Model object\nWe need to define the Model object using the properties we defined so far:\n", "metadata": {}}, {"cell_type": "code", "source": "from ibm_watsonx_ai.foundation_models import ModelInference\n\nmodel = ModelInference(\n\tmodel_id = model_id,\n\tparams = parameters,\n\tcredentials = get_credentials(),\n\tproject_id = project_id,\n\tspace_id = space_id\n\t)\n", "metadata": {"id": "0b6121f2-8f8f-4d02-b301-2c85dc9b3b31"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Defining the inferencing input\nFoundation model inferencing API accepts a natural language input that it will use\nto provide the natural language response. The API is sensitive to formatting. Input\nstructure, presence of training steps (one-shot, two-shot learning etc.), as well\nas phrasing all influence the final response and belongs to the emerging discipline of\nPrompt Engineering.\n\nLet us provide the input we got from the Prompt Lab:\n", "metadata": {}}, {"cell_type": "code", "source": "prompt_input = \"\"\"Write a short summary for the meeting transcripts.\n\nTranscript: 00:00   [John]   I wanted to share an update on project X today.\n00:15   [John]   Project X will be completed at the end of the week.\n00:30   [Jane]  That's great!\n00:35   [Jane]  I heard from customer Y today, and they agreed to buy our product.\n00:45   [Joe]  Customer Z said they will too.\n01:05   [John]   Great news, all around.\nSummary: John shared an update that project X will be completed end of the week and will be purchased by customers Y and Z.\n\nTranscript: 00:00   [Jane]   The goal today is to agree on a design solution.\n00:12   [John]  I think we should consider choice 1.\n00:25   [Jane]   I agree\n00:40   [Joe]  Choice 2 has the advantage that it will take less time.\n01:03   [John]  Actually, that's a good point.\n01:30   [Jane]   So, what should we do?\n01:55   [John]  I'm good with choice 2.\n02:20   [Joe]  Me too.\n02:45   [Jane] Done!\nSummary: Jane, John, and Joe decided to go with choice 2 for the design solution because it will take less time.\n\nTranscript: 1\n\nJohn Doe 00:00:01.415 --> 00:00:20.675\n\nA little bit of context of the email from last night, I've been working on the data from the feature set that came out of the analysis last time.\n\n2\n\nJohn Doe 00:00:21.334 --> 00:00:42.394\n\nThanks for sharing that. We were trying to understand the impact of changing levels of discretization, um, on both our data.\n\n3\n\nJohn Doe 00:00:42.784 --> 00:01:03.454\n\nAnd the original data, um, and 1 of the main things we were trying to to figure it out is, uh, because we did not have access to the notebook, uh, that you ran the analysis with was to how can we reproduce some of the results and in doing so I think we found something sort of fairly critical, which, which it was.\n\n4\n\nJohn Doe 00:01:03.664 --> 00:01:24.694\n\nAnswered earlier this morning, but, um, it's sort of prohibited us from continuing any further analysis in the sense that it seems that the, uh, evaluation of the testing, uh, of the downstream classifier was somehow done on the training data. Um, and the problem is.\n\n5\n\nJohn Doe 00:01:24.699 --> 00:01:45.004\n\nThat if we, um, I mean, obviously, we will have to rectify that, because it's, it's not informative fee kind of, uh, you know, use the training data for testing. But, um, I mean, if if that were in the protocol, just switching the level of visualization of the method.\n\n6\n\nJohn Doe 00:01:46.774 --> 00:02:06.754\n\nUm, to 10 minutes, that was, I think the case in the original setup, uh, takes the to 95% and I think that's kind of what triggered us to understand that that cannot be happening. Um, so 1 of the objectives, uh, if, if the group agrees for today's meeting, I think maybe we could just set up a.\n\n7\n\nJohn Doe 00:02:07.414 --> 00:02:28.114\n\nUm, outline for a consistent evaluation protocol that we can replicate on our side as well. And then, um, we essentially can replicate, uh, reproduce the, the, you know, as long as we can ensure, we can review the numbers. Um, we could just repeat our.\n\n8\n\nJohn Doe 00:02:28.174 --> 00:02:37.204\n\nAnalysis and in the next meeting shared that does that does that sound good to do? Are there any questions 1st of all? I think maybe I should ask that.\n\n9\n\nJohn Doe 00:02:47.555 --> 00:03:06.215\n\nAwesome, no, no questions then in that case, does that sound good to maybe, uh, go through the notebook a little bit, uh, in detail this time to understand the evaluation protocol um, and then we have certain suggestions that, uh, we could make about.\n\n10\n\nJohn Doe 00:03:06.244 --> 00:03:17.884\n\nUm, how we, uh, currently evaluate, um, to just avoid, uh, information leakage from training into the test would that be okay?\n\n11\n\nJane Doe 00:03:18.304 --> 00:03:25.834\n\nAnd maybe a, just to summarize for everyone, it sounds like you ran you rebound the tests that they were doing.\n\n12\n\nJane Doe 00:03:27.484 --> 00:03:31.594\n\nThe team has already done and achieved, uh.\n\n13\n\nJane Doe 00:03:31.714 --> 00:03:46.924\n\nOur score point 95 or something, so quite high, but suggestions that that probably isn't the best marker to evaluate and so we want to go back through it and make some suggestions on the overall approach.\n\n14\n\nJohn Doe 00:03:47.644 --> 00:03:52.534\n\nThat's right. That's right. I think I think the only just to kind of get the, the idea of, like, what we change.\n\n15\n\nJohn Doe 00:03:52.774 --> 00:04:13.894\n\nIf we change the level of discretization of our model to the level of discretization that, uh, was in the notebook for the original data, and then follow the same testing training protocol, which I must say is not, uh, we do get 95% but primarily, because we are testing on training data.\n\n16\n\nJohn Doe 00:04:13.924 --> 00:04:15.154\n\nIs not the right thing to do?\n\n17\n\nJane Doe 00:04:18.604 --> 00:04:22.564\n\nYou train the data are using, uh, the the same target.\n\n18\n\nJohn Doe 00:04:23.614 --> 00:04:24.334\n\nYes.\n\n19\n\nJohn Doe 00:04:30.634 --> 00:04:47.734\n\nI mean, we just, we, we, we were just trying to replicate the results. So I think the my main concern is that the results that maybe we shared last time WH, what were shared with last time. They, we can't. Those are not right in some sense, right? The, the results were done by training and testing.\n\n20\n\nJohn Doe 00:04:47.764 --> 00:04:48.664\n\nOn the same dataset.\nSummary:\"\"\"\n", "metadata": {"id": "c98b8de1-f016-48ad-a18c-27fce7cf3bfb", "jupyter": {"source_hidden": true}}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Execution\nLet us now use the defined Model object and pair it with input and\ngenerate the response:\n", "metadata": {}}, {"cell_type": "code", "source": "print(\"Submitting generation request...\")\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)\n", "metadata": {"id": "1fc997ce-fea5-446e-8402-fe27a9ebd9a6"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Prompt Engineering Exercises\nIt\\'s time to get prompting - complete the following exercise to hone your prompt engineering skills.\n\nYou will be given a prompt template (e.g. a customer review) and an objective. Your task is to engineer the prompt to achieve the specified target. \n\n**Note**: Keep iterating and testing your prompt, try changing the model and/or using non-default parameters to achieve your goal.", "metadata": {"id": "2b365728-84e0-4277-9854-537f54c50583"}}, {"cell_type": "code", "source": "# The following is a Product Review. This review will be used for Questions 1 - 5\n\nreview = \"\"\"Needed a nice lamp for my bedroom, and this one had additional storage and not too high of a price point. Got it fast. \\\nThe string to our lamp broke during the transit and the company happily sent over a new one. Came within a few days as well. It was easy to put \\\ntogether.  I had a missing part, so I contacted their support and they very quickly got me the missing piece! Lumina seems \\\nto me to be a great company that cares about their customers and products!!\"\"\"", "metadata": {"id": "0d253d66-c233-4566-9305-feaf60335a2b"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q1) write a prompt to return the sentiment of the review\n\nTarget sentiment = positive", "metadata": {"id": "e16a29e9-0f92-4aea-83ee-48bd5837f982"}}, {"cell_type": "code", "source": "prompt_input = f\"\"\"Instruction:\n\nreview: ```{review}```\"\"\"\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)", "metadata": {"id": "b91f7700-f582-473d-83cb-2f9e11f5c4fb"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q2) extract the emotions the reviewer expressed, return answer as a comma separated list\n\nTarget emotions = satisfied, happy, cared for, great company, product!", "metadata": {"id": "a430cc58-69a3-49f4-9cdd-58cc087aa21a"}}, {"cell_type": "code", "source": "prompt_input = f\"\"\"Instruction:\n\nreview: ```{review}```\"\"\"\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)", "metadata": {"id": "3830ccd9-ccb8-4247-a681-a0b2feaf5416"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q3) Is the reviewer expressing anger, answer \u201cyes\u201d or \u201cno\u201d \u2013 test with your own example including anger to ensure it works in both cases.\n\nTarget answer = no", "metadata": {"id": "e7abe424-9e3c-45f7-ab8a-4d8ee6dc016a"}}, {"cell_type": "code", "source": "prompt_input = f\"\"\"Instruction:\n\nreview: ```{review}```\"\"\"\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)", "metadata": {"id": "5ca060d0-a180-4256-a3b9-6061706081b7"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q4) Extract the item purchased and the company name, return as JSON format\n\nTarget answer = Item[lamp], Brand[Lumina]", "metadata": {"id": "991f7239-ab02-4e97-ae4c-4a348fad6f2d"}}, {"cell_type": "code", "source": "prompt_input = f\"\"\"Instruction:\n\nreview: ```{review}```\"\"\"\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)", "metadata": {"id": "18af1916-be18-418e-badf-fd8c0704ec98"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q5) Can you combine 3-6 in a single prompt and return JSON with: Sentiment (negative or positive), Anger (yes/no), Product, Company\n\nTarget answer = Sentiment[positive], Anger[false], Item[lamp], Brand[Lumina]", "metadata": {"id": "4190cc1d-b5ec-4553-b11a-390a9bd9b09d"}}, {"cell_type": "code", "source": "# The following is a Product Review. This review will be used for Questions 8 - 8\nreview = \"\"\"Got this panda plush toy for my daughter's birthday, who loves it and takes it everywhere. It's soft and \\ \nsuper cute, and its face has a friendly look. It's a bit small for what I paid though. I think there might be other \\\noptions that are bigger for the same price. It arrived a day earlier than expected, \\\nso I got to play with it myself before I gave it to her.\"\"\"", "metadata": {"id": "dff717f9-9ddf-45cd-8918-8c2bcdc3d7a9"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q6) summarize the following product review\n\nTarget summary = My daughter loves it! It's soft and super cute, and its face has a friendly look. It's a bit small for what I paid though.", "metadata": {"id": "df44d711-42d3-4fe0-942d-7d2973ca3f98"}}, {"cell_type": "code", "source": "prompt_input = f\"\"\"Instruction:\n\nreview: ```{review}```\"\"\"\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)", "metadata": {"id": "bf8107b1-6e79-49ef-8e18-1fc2c09760ef"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q7) Summarize the same product review from the perspective of the shipping department\n\nTarget summary = It arrived a day earlier than expected, so I got to play with it myself before I gave it to her.", "metadata": {"id": "325c9885-b22d-4df0-993c-685c0476a1f3"}}, {"cell_type": "code", "source": "prompt_input = f\"\"\"Instruction:\n\nreview: ```{review}```\"\"\"\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)", "metadata": {"id": "e9084324-de25-4520-b4e0-14feb1faea7b"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q8) Summarize the review from the perspective of pricing and value\n\nTarget summary = It's a bit small for what I paid though. I think there might be other options that are bigger for the same price", "metadata": {"id": "2f12df20-9167-452e-b5be-d5aca2fdea60"}}, {"cell_type": "code", "source": "prompt_input = f\"\"\"Instruction:\n\nreview: ```{review}```\"\"\"\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)", "metadata": {"id": "d4585191-fe86-425a-ae60-64bd8317cd9b"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# This email contains sensitive Personal Identifiable Information (PII).\nemail = \"\"\"Hi John,\\\n\nI'm writing to you because I noticed you recently purchased a new car. I'm a salesperson\\\nat a local dealership (Cheap Dealz), and I wanted to let you know that we have a great deal on a new\\\ncar. If you're interested, please let me know.\\\n\nThanks,\\\n\nJimmy Smith\\\n\nPhone: 410-805-2345\\\nEmail: jimmysmith@cheapdealz.com\"\"\"", "metadata": {"id": "be035cf6-bccb-419c-bac5-3d6b9b32e052"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q9) Personal Identifiable Information (PII) removal. Given the following email, write a prompt to remove the PII (e.g., names, emails etc)\nHint: you may need to use 1-2 shot technique", "metadata": {"id": "723f3d9a-c0a2-4fcc-b935-d02ccf001e83"}}, {"cell_type": "code", "source": "prompt_input = f\"\"\"Instruction:\n\ninput: ```{email}```\n\noutput:\"\"\"\ngenerated_response = model.generate_text(prompt=prompt_input, guardrails=True)\nprint(generated_response)", "metadata": {"id": "4e97435b-507e-42a1-8e23-d4b057a2ceb6"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "#### Q10) Challenge: A patients a1c level determines their diabetes status, the rules are as follows:\n\n- less than 5.7 no diabetes\n- between 5.7 and 6.5 pre-diabetes\n- greater than 6.5 diabetic.\n\n**Task: Write a prompt to return just the diabetes status from the following 3 test cases:**\n\n1. The patients a1c is 5.5 which is good considering his other risk factors.\n2. From the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\n3. She mentioned her A1c is 8 according to her blood work about 3 years ago.\n\n**Stretch 1**: How could you improve the inference given the other information in the sentences?\n\n**Stretch 2**: How would you approach extracting the diabetes status based on patient notes without A1C values and what would you need to watch out for? (hint: maybe they are talking about family history of disease or other complications)\n", "metadata": {"id": "3711c2f1-4dc6-4762-87ff-d106950409bc"}}, {"cell_type": "code", "source": "# Note; produce a single prompt that works for ALL of the following records\n\nrecords = [\"The patients a1c is 5.5 which is good considering his other risk factors.\", \n           \"From the last lab report I noted the A1c is 6.4 so we need to put her on Ozempic.\", \n           \"She mentioned her A1c is 8 according to her blood work about 3 years ago.\"]", "metadata": {"id": "271e7070-75b3-4d8d-9459-35c676bf770d"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "for record in records:\n    prompt_input = f\"\"\"Instruction:\n    \n    input: ```{record}```\n    \n    output:\"\"\"\n    \n    print(f\"Record: {record}\")\n    generated_response = model.generate_text(prompt=prompt_input, guardrails=True)\n    print(generated_response)\n    print(\"******\\n \")", "metadata": {"id": "fd7d4105-77b5-46c6-a8b0-bd7c2440b92a"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Well done!\nYou successfully completed this notebook! You learned how to use\nwatsonx.ai inferencing SDK to generate response from the foundation model\nbased on the provided input, model id and model parameters. Check out the\nofficial watsonx.ai site for more samples, tutorials, documentation, how-tos, and blog posts.", "metadata": {"id": "692a25f7-9e5e-4105-be3d-844f1edfb86a"}}]}